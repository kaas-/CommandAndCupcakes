\chapter{The experiment}\label{ch:experiment}
The game described in Chapter \ref{ch:game} serves as a tool to test the problem formulation:

\begin{changemargin}{1cm}{1cm}
\textbf{How can the player’s attention be directed between a shared screen and a personal hand-held screen when there is a need to change their visual focus between the two in a local multiplayer game?}
\end{changemargin}

Since the focus of this problem formulation is on directing attention between two screens in a setup such as that of the AirConsole, the experiment described in this chapter sought to discover which method is best fit for directing the players' attention. To do this, test participants played each of the four different versions of the game, which as mentioned in Chapter \ref{ch:game}, utilise four different methods of directing attention. The effects on the players' attention were measured through a combination of self-report questions and gaze tracking. This chapter describes the experiment in further detail.

\section{Metrics}
In order to test the problem formulation, a combination of several metrics were measured. These metrics may be split into two categories: performance and user experience metrics. Both are described in this section.

\subsection{User experience metrics}\label{subsec:user_experience_metrics}
After playing each version of the game, test participants were asked to fill out a questionnaire. The answers from this questionnaire serve as user experience metrics. As this is self-reported data from the participants, it is fit for measuring their own experience. The questions in the questionnaire were as follows:

\pagebreak
\begin{enumerate}
	\item To which degree do you agree with the following statements?
	\begin{enumerate}
		\item It was clear when it was my turn to act
		\item It was clear when I was in combat
		\item It was clear when I was supposed to look at the phone screen
		\item It was clear when I was supposed to look at the main screen
		\item It was easy for me to switch my attention between the two screens
		\item It was easy to find what I was looking for when switching my gaze between the screens
	\end{enumerate}
	\item What did you think of the dual-screen setup? What did you enjoy about it? What did you find confusing?
	\item Do you have any additional notes?
\end{enumerate}

The rating scale statements in Question 1 provide seven possible answers: strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, and strongly agree. The inclusion of the \textit{neutral} option allows participants who neither agree nor disagree with a statement to pick a fitting option, rather than being forced to decide whether they agree or disagree, which may lessen the validity of the test. The inclusion of the \textit{somewhat disagree} and \textit{somewhat agree} options allows participants who feel that they only partially agree with a statement to pick a side without sounding too opinionated, which may dissuade some from picking the completely neutral option in these cases.

For each of these six statements, the answers gathered can be used to answer the following hypothesis:

\begin{changemargin}{1cm}{1cm}
\textbf{Test participants agree with the statement to a higher degree for this version of the game than for the control version.}
\end{changemargin}

This hypothesis is addressed further into this chapter, where it is answered for each of the four versions of the game. The control version of the game is the one with no stimuli. This version is labelled version A in Chapter \ref{ch:game}.

\subsection{Performance metrics}\label{subsec:performance_metrics}
Performance metrics measure concrete results observed throughout the experiment. While playing the game, players were recorded both with a video camera and a Kinect 2, where the latter logged when the players looked at the main screen and when they looked away. The game software logged when it was a new player's turn and when combat was initiated. By comparing time stamps from these logs, it was possible to measure how much time passed from the moment it became a player's turn, to the moment they looked at their smartphone. In addition to this, error rates were also measured; for each turn, it was noted whether a participant looked at their screen at all, or if a player was already looking at their screen when their turn started. 

\section{Setup}
Each test included four participants. Apart from these, three people were present to facilitate the test:

\begin{itemize}
	\item A \textbf{test conductor}, who instructed the participants in the terms of the test
	\item A \textbf{note taker}, who wrote down anything of importance
	\item A \textbf{technician}, who made sure that both the Kinect 2 and the game were running smoothly, and that the camera was on.
\end{itemize}

The following materials were used for the purpose of this test:

\begin{itemize}
	\item A television screen
	\item A laptop that ran the game, connected to the TV screen through an HDMI cable
	\item A Kinect 2
	\item A laptop that ran the Kinect Studio 2.0 software, while connected to the Kinect 2 through a USB 3.0 cable
	\item Four smartphones
	\item A video camera
	\item One consent form for each participant
	\item Four questionnaires for each participant
	\item Four different version of the game
\end{itemize}

These materials were set up as shown in Figure \ref{fig:test_setup}. The four participants were seated in front of the television screen showing the game. This way, the main screen for all four participants was the television screen. Underneath this screen was the Kinect 2 which was aimed at the participants. In front of each participant was a smartphone, which was connected to the game.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/test_setup.png}
	\caption{Test setup, showing the television screen (A), the Kinect 2 (B), a computer running the game (C), a computer running the Kinect Studio 2.0 software (D), seating for participants (E), and four smartphones (F)}\label{fig:test_setup}
\end{figure}

\section{Method}
Once participants were seated, the test conductor explained the test procedure as well as the basics of the game. Once every participant was ready, the game began, and participants played freely until a the game ended or five minutes had passed, whichever came first. While the game was ongoing, the participants were filmed with the video camera, and data about their gaze was recorded with the Kinect 2. Each group of participants played through the four versions of the game, in a randomised order.

After a group of four participants had tried a version of the game, every participant was asked to fill out the questionnaire described in Section \ref{subsec:user_experience_metrics}. While the game was ongoing, the Kinect 2 logged whether players looked at or away from the main screen, and the game logged whose turn it was at any given time.

\section{Results}
\subsection{User experience results}
After having played a version of the game, each participant was asked to fill out the questionnaire described in Section \ref{subsec:user_experience_metrics}. The distribution of answers given for Question 1 are illustrated in Figures \ref{fig:questionnaire_a} through \ref{fig:questionnaire_f}.
%at which participants agreed with the statements given in Question 1 will be described in this section. The distribution of answers given in Statement 1(a) are illustrated in Figure \ref{fig:questionnaire_a}. From this figure, it is clear that participants most strongly agreed with the notion that it was clear when it was their turn to act when given tactile stimulus, or visual stimulus on the main screen. They found it most unclear when given nu stimulus at all.

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_a.png}
	\caption{Answers for the question "it was clear when it was my turn to act" for each of the four versions}\label{fig:questionnaire_a}
\end{figure}

%Figure \ref{fig:questionnaire_b} shows that it was somewhat clearer to participants when they were in combat when given no stimulus, but similarly to the previous question participants mostly found it clear when given tactile stimulus or visual stimulus on the main screen.

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_b.png}
	\caption{Answers for the question "it was clear when I was in combat" for each of the four versions}\label{fig:questionnaire_b}
\end{figure}

%Similar results are seen in Figure \ref{fig:questionnaire_c}, where players were least in doubt regarding when they were supposed to look at the phone screen when given main screen or tactile stimuli. However when looking at the answers seen in Figure \ref{fig:questionnaire_d}, players were generally neutral regarding whether it was clear when they should be looking at the main screen. The version with tactile feedback stands out in this case as the version where most people found it clear when they should look at the main screen.

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_c.png}
	\caption{Answers for the question "it was clear when I was supposed to look at the phone screen" for each of the four versions}\label{fig:questionnaire_c}
\end{figure}

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_d.png}
	\caption{Answers for the question "It was clear when I was supposed to look at the main screen" for each of the four versions}\label{fig:questionnaire_d}
\end{figure}

%Figure \ref{fig:questionnaire_e} illustrates to which degree participants found it easy to switch their attention between the two screens in the four different versions of the game. The results here show that people especially found it easy in the versions with tactile and main screen stimuli.

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_e.png}
	\caption{Answers for the question "It was easy for me to switch my attention between the two screens" for each of the four versions}\label{fig:questionnaire_e}
\end{figure}

%Figure \ref{fig:questionnaire_f} shows that players generally found it easy to find what they were looking for when switching their gaze between the two screens. These results stand out compared to those of the previous questions, as they do not seem to be affected as much by the stimulus given to the participants when it was their turn, and when they are engaged in combat; even the versions with no stimulus, or visual stimulus on the phone screen, got more positive results on this question.

\begin{figure}[hp!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_f.png}
	\caption{Answers for the question "It was easy to find what I was looking for when switching my gaze between the two screens" for each of the four versions}\label{fig:questionnaire_f}
\end{figure}

\subsection{Performance results}
As mentioned in Section \ref{subsec:performance_metrics}, two logs were generated throughout the test: one noting time stamps for different events in the game, and one noting data from the Kinect 2 regarding whether players looked at or away from the main screen. By comparing the two, it was possible to measure the amount of time it took for each player to look at their phone after it became their turn, as well as after they were engaged in combat. This reaction time was noted for each of these events, and an average reaction time was calculated for each of the four versions of the game. As can be seen in Figure \ref{fig:reaction_time}, participants switched their visual attention fastest when given tactile stimulus, at an average of 0,78 seconds. Players switched their visual attention slowest when given visual stimulus on the main screen.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/reaction_time.png}
	\caption{Average reaction time for the four versions of the game}\label{fig:reaction_time}
\end{figure}

It is, however, significant to note that in some cases, test participants failed to look at the phone screen at all during their turn or a combat they were a part of, and in other cases, participants were already looking at their phone by the time it became their turn or they engaged in combat. In these cases, it was impossible to note a reaction time. Instead, Figure \ref{fig:looked} illustrates the rate at which these two circumstances occur for each of the four versions. As the figure shows, these cases occurred most frequently when players were given no indication of when it was their turn, and least so when they were given visual stimulus on the main screen. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/graph_looked.png}
	\caption{The rate at which participants failed to look at their phone screen, or were already looking at it, for each of the four versions}\label{fig:looked}
\end{figure}

\section{Analysis}
The results from the questionnaires were set up in a table for each of the statements described in Section \ref{subsec:user_experience_metrics}, and each possible answer was given a value ranging from 1-7, where 1 corresponds to a "strongly disagree" response, and 7 corresponds to "strongly agree". This was done to make it possible to use a Wilcoxon ranked sum test, to see if there is a significant difference between the game version with no stimuli and the game versions with stimuli. In this case, version A serves as the control version. The Wilcoxon ranked sum test fits the test since it aims to find differences between two paired samples with a nominal predictor variable (the test version), and a single ordinal response variable (the questionnaire reponses from the participants), along with the fact that the data is non-parametric with a sample size below 30. The Wilcoxon test is made for each statement in the questionnaire, where each non-control sample is compared to the control sample to test the following hypothesis:

\begin{changemargin}{1cm}{1cm}
\textbf{$H_a$ :} Test participants agree with the statement to a higher degree for this version of the game than for the control version.
\end{changemargin}

The corresponding null hypothesis is:

\begin{changemargin}{1cm}{1cm}
\textbf{$H_0$ :} Test participants do not agree with the statement to a higher degree for this version of the game than for the control version.
\end{changemargin}

The p-value for each of these 18 tests are seen in Table \ref{tbl:p_table}. All of the cases where p < 0.05 are marked in \textbf{bold}.

\begin{center}
  \begin{tabular*}{\textwidth}{ | p{4.84cm} | p{2.5cm} | p{2.5cm} | p{2.52cm} |}
    \hline
    \textit{Game version} & \textit{B) Tactile} & \textit{C) Main screen visual} & \textit{D) Phone screen visual} \\ \hline
    It was clear when it  was my turn to act & \textbf{p = 0.0006928} & \textbf{p = 0.00006759} & \textbf{p = 0.009489} \\ \hline
    It was clear when I was in combat & \textbf{p = 0.0133} & \textbf{p = 0.005767} & p = 0.5524 \\ \hline
    It was clear when I was supposed to look at the phone screen & \textbf{p = 0.00494} & \textbf{p = 0.00001718} & p = 0.16 \\ \hline
    It was clear when I was supposed to look at the main screen & p = 0.1028 & \textbf{p = 0.0002686} & p = 0.3058 \\ \hline
    It was easy for me to switch my attention between the two screens & \textbf{p = 0.007815} & \textbf{p = 0.04137} & p = 0.3266 \\ \hline
    It was easy to find what I was looking for when switching my gaze between the screens & p = 0.05888 & \textbf{p = 0.04512} & p = 0.1975 \\
    \hline
  \end{tabular*}\captionof{table}{p-values for each test}\label{tbl:p_table}
\end{center}

When looking at the p-values from version B, it is clear that the tactile stimulus helped the participants in noticing when it was their turn, when they entered combat, and when to look at their phone compared to the control version. It also helped them to switch their attention between the screens. However, this feedback type was lacking when it came to directing the attention to the main screen. 
The p-values from version C show that all of the hypotheses are true and that the null-hypothesis can be rejected. This means that this type of feedback works significantly better than the control sample in all areas addressed by the questionnaire.
The results for version D show a significant improvement compared to the control version only in one area; communicating to the participants when it was their turn to act. This indicates that it did not help to direct attention as well as the other two versions. 

From these results, it can be seen that version C in general performs better when it comes to user experience metrics than other types of feedback. The version with the poorest user-experience results is version D, where only 1 of the 6 p-values are below the 0.05 margin.

When looking at the performance metric results, it is important to note that even though the game version with visual stimulus on the main screen caused test participants to perform the worst with regards to reaction time, it is the version which caused fewest cases where participants were already looking at the screen to wait for their turn. The version with no indication of whose turn it is caused relatively faster reactions from participants, but many would constantly look at their phone screen, even if it was not their turn, which is also not ideal. This is possibly because they were anticipating their turn, but since they would be given no indication of when their turn would start, they had to check in with their phone screen more often. This would cause them to look at the main screen less. Version B performed the best with regards to reaction time, as it is the only version in which participants on average switched their gaze from the main screen to the phone screen in less that a second.

\section{Discussion of results}
Even though the evaluation yielded data to draw conclusions from, there is a need to take certain aspects into account that might have influenced the validity and reliability of results both in a positive and negative manner. 

\subsection{Reliability}
The test was designed in such a way that each participant tested all of the four versions of the game. This is a positive aspect of the evaluation setup, since that way the feedback given on all of the game versions did not depend on each version being viewed by different people.

On the negative side, time measurements, both in Kinect and game log, did not include milliseconds, since it would be difficult to have the exact same time on two computers. The solution to this issue could have been to implement the Kinect code within the game software, which was not possible to do due to time constraints.

Furthermore, there were four different models of phones in use during a test, and one of the phones was switched for the last three tests, totalling to five different models used. Therefore, the consistency of the test was influenced.

\subsection{Validity}
There were several factors that influenced validity in a positive way, which include having different groups of participants play the four versions of the game in a randomised order and providing a test environment that resembles a home setup. The latter may have influenced the participants’ level of comfort and helped reduce the background noise level. 

On the negative side, the test was conducted only among third and fifth semester Medialogy students, which may have led to biased results. In addition, it would have been ideal to have more participants since there were four versions of the game. Even though the order in which the versions were played was randomised between the groups, in order to test every possible playing order, 24 tests in groups of four participants should have been conducted. This would sum up to involving 96 participants, which would not be possible to do considering the time constraints.

The results may have been affected in a negative way due to some technical issues. For instance, there was latency during combat, and in a few cases, the necessary buttons did not appear at once. Also, the tactile stimulus was not strong enough on some of the phones during the test according to some participants, thus making those tactile results invalid. It may have been ideal for the vibrations to last longer, allowing more participants to notice them.

Another source of error might have occurred due to the results obtained from the Kinect 2, since it might not have been precise enough for the purpose of the evaluation testing. Because of this, the software may have logged that a participant was looking away when he or she was looking at the main screen and vice versa. Furthermore, there were cases where it failed to register a change in gaze at all. This could have happened due to the position of the hardware, which was right below the main screen, so the participants did not look directly on the Kinect 2 camera, but at a point above it. Another source of error could be that different test participants bend their necks to different degrees when looking at their smartphone.

Furthermore, tracking what the participants were specifically concentrating on when looking at the main screen could have given us a better understanding of how the attention of the participants was directed. 

\section{Conclusion of experiment}
When considering the results from both the user experience and the performance tests, it is clear that in several aspects, a visual cue on the shared screen is a viable way to direct players' attention between two screens in a setup such as that of the AirConsole platform. This stands in contrast to a visual stimulus on the phone screen, which shows little indication that it helps players in directing their attention to the relevant screen. However, tactile stimulus also shows potential in directing players' attention, especially if the situation warrants a quick reaction. Since tactile and visual stimuli are not in direct conflict with each other, a viable solution may be to combine phone vibration with a visual cue on the main screen. This project does not investigate the performance of such a solution, and thus a final conclusion on that subject would need further research. This, as well as additional potential for further research, will be discussed in the following chapter.