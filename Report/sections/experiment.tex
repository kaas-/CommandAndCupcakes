\chapter{The experiment}\label{ch:experiment}
The game described in Chapter \ref{ch:game} serves as a tool to test the problem formulation:

\begin{changemargin}{1cm}{1cm}
\textbf{How can the playerâ€™s attention be directed between a shared screen and a personal hand-held screen when there is a need to change their visual focus between the two in a local multiplayer game?}
\end{changemargin}

Since the focus of this problem formulation is on directing attention between two screens in a setup such as that of the AirConsole, the experiment described in this chapter sought to discover which method is best fit for directing the players' attention. To do this, test participants were subjected to the four different iterations of the game, which as mentioned in Chapter \ref{ch:game}, utilise four different methods of directing attention. The effect on the player's attention were measured through a combination of self-report questions and gaze tracking. This chapter describes the experiment in further detail.

\section{Metrics}
In order to test the problem formulation, a combination of several metrics were measured. These metrics may be split into two categories: performance and user experience metrics. Both will be described in the following subsections.

\subsection{User experience metrics}\label{subsec:user_experience_metrics}
After playing an iteration of the game, test participants were asked to fill out a questionnaire. The answers from this questionnaire serve as user experience metrics. As this is self-reported data from the participants, it is fit for measuring the users' own experience, but less so for measuring their concrete performance. However, as their experience is still relevant to the problem formulation, this experiment made use of self-report questionnaires. The questions in the questionnaire were as follows:

\begin{enumerate}
	\item To which degree do you agree with the following statements?
	\begin{enumerate}
		\item It was clear when it was my turn to act
		\item It was clear when I was in combat
		\item It was clear when I was supposed to look at the phone screen
		\item It was clear when I should look at the main screen
		\item It was easy for me to switch my attention between the two screens
		\item It was easy to find what I was looking for when switching my gaze between the screens
	\end{enumerate}
	\item What did you think of the dual-screen setup? What did you enjoy about it? What did you find confusing?
	\item Do you have any additional notes?
\end{enumerate}

The rating scale statements in question 1 provide seven possible answers: Strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, and strongly agree. The inclusion of the \textit{neutral} option allows participants who neither agree nor disagree with a statement to pick a fitting option, rather than being forced to decide whether they agree or disagree, which may damage the validity of the test. The inclusion of the \textit{slightly agree} and \textit{slightly disagree} options allow participants who feel that they only partially agree with a statement to pick a side without sounding too opinionated, which may dissuade some from picking the completely neutral option in these cases.

For each of these six statements, the answers gathered can be used to answer the following hypothesis:

\begin{changemargin}{1cm}{1cm}
\textbf{Test participants agree with the statement to a higher degree for this version of the game than for the control version.}
\end{changemargin}

This hypothesis will be adressed further into this chapter, where it is answered for each of the four versions of the game. The control version of the game is the version with no indication of when it is a player's turn, or when they are in combat. This version is labelled version A in Chapter \ref{ch:game}.

\subsection{Performance metrics}\label{subsec:performance_metrics}
Performance metrics measure concrete results observed throughout the experiment. While playing the game, players were recorded both with a video camera and a Kinect 2, where the latter logged when the players looked at the main screen and when they looked away. The game software logged when it was a new player's turn. By comparing time stamps from these logs, it was possible to measure how much time passed from the moment it became a player's turn, to the moment they looked at their smart phone. The time stamps logged by the game also allowed for the measurement of performance time, namely the time it takes for a participant to complete their turn. In addition to this, error rates were also measured; for each turn, it was noted whether a participant looked at their screen at all	 and whether they completed their turn before the given time ran out. 

\section{Setup}
Each test included four participants. Apart from these, three people were present to facilitate the test:

\begin{itemize}
	\item A \textbf{test conductor}, who instructed the participants in the terms of the test
	\item A \textbf{note taker}, who wrote down anything of note
	\item A \textbf{technician}, who made sure that both the Kinect 2 and the game were running smoothly, and that the camera was on.
\end{itemize}

Apart from the test conductor, the people facilitating the test remained as silent as possible throughout the test in order to not interfere with its results. The following materials were used for the purpose of this test:

\begin{itemize}
	\item A television screen
	\item A computer that runs the game, connected to the TV screen through an HDMI cable
	\item A Kinect 2
	\item A computer that runs the Kinect, connected to it through a USB 3.0 cable
	\item 4 smartphones
	\item A video camera
	\item One consent form for each participant
	\item Four questionnaires for each participant
	\item Four different builds of the game
\end{itemize}

These materials were set up as shown in Figure \ref{fig:test_setup}. The four participants were seated in front of a large television screen, to which a computer running the game was connected through an HDMI cable. This way, the main screen for all four participants was the television screen. Underneath this screen was a Kinect 2 which was aimed at the participants while connected to another computer through a USB 3.0 cable. In front of each participant was a smartphone, which was connected to the game.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/test_setup.png}
	\caption{Test setup, showing the television screen (A), the Kinect 2 (B), a computer running the game (C), a computer running the Kinect 2 (D), seating for participants (E), and 4 smartphones (F)}\label{fig:test_setup}
\end{figure}

\section{Method}
Once participants were seated, the test conductor explained the test procedure as well as the basics of the game. Once every participant was ready, the game began, and participants played freely until a the game ended or 10 minutes had passed, whichever came first. While the game was ongoing, the participants were filmed with the video camera, and data about their gaze was recorded with the Kinect 2. Each group of participants played through four iterations of the game, in a randomised order. The four iterations, as mentioned in Chapter \ref{ch:game}, utilise four different methods for directing the player's attention to their phone screen when it becomes their turn, as well as when they are engaged in combat. These four methods are:

\begin{enumerate}[label=\Alph*)]
	\item No feedback
	\item Tactile feedback (phone vibration
	\item Visual feedback (warning image on main screen)
	\item Visual feedback (warning image on phone screen)
\end{enumerate}

After a group of four participants had tried an iteration of the game, every participant was asked to fill out the questionnaire described in Section \ref{subsec:user_experience_metrics}. While the game was ongoing, the Kinect logged whether players looked at or away from the main screen, and the game logged whose turn it was at any given time. The note taker noted down each player's error rates as described in Section \ref{subsec:performance_metrics}.

\section{Results}
\subsection{User experience results}
After having played a version of the game, each participants as asked to fill out the questionnaire described in section \ref{subsec:user_experience_metrics}. The rate at which participants agreed with the statements given in question 1 will be described in this section. The answers given in statement 1(a) are illustrated in Figure \ref{fig:questionnaire_a}. From this figure, it is clear that participants most strongly agreed with the notion that it was clear when it was their turn to act when given tactile stimulus, or visual stimulus on the main screen. They found it most unclear when given nu stimulus at all.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_a.png}
	\caption{Answers for the question "it was clear when it was my turn to act" for each of the four versions}\label{fig:questionnaire_a}
\end{figure}

Figure \ref{fig:questionnaire_b} shows that it was somewhat clearer to participants when they were in combat when given no stimulus, but similarly to the previous question participants mostly found it clear when given tactile stimulus or visual stimulus on the main screen.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_b.png}
	\caption{Answers for the question "it was clear when I was in combat" for each of the four versions}\label{fig:questionnaire_b}
\end{figure}

Similar results are seen in Figure \ref{fig:questionnaire_c}, where players were least in doubt regarding when they were supposed to look at the phone screen when given main screen or tactile stimuli. However when looking at the answers seen in Figure \ref{fig:questionnaire_d}, players were generally neutral regarding whether it was clear when they should be looking at the main screen. The version with tactile feedback stands out in this case as the version where most people found it clear when they should look at the main screen.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_c.png}
	\caption{Answers for the question "it was clear when I was supposed to look at the phone screen" for each of the four versions}\label{fig:questionnaire_c}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_d.png}
	\caption{Answers for the question "It was clear when I was supposed to look at the main screen" for each of the four versions}\label{fig:questionnaire_d}
\end{figure}

Figure \ref{fig:questionnaire_e} illustrates to which degree participants found it easy to switch their attention between the two screens in the four different versions of the game. The results here show that people especially found it easy in the versions with tactile and main screen stimuli.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_e.png}
	\caption{Answers for the question "It was easy for me to switch my attention between the two screens" for each of the four versions}\label{fig:questionnaire_e}
\end{figure}

Figure \ref{fig:questionnaire_f} shows that players generally found it easy to find what they were looking for when switching their gaze between the two screens. These results stand out compared to those of the previous questions, as they do not seem to be affected as much by the stimulus given to the participants when it was their turn, and when they are engaged in combat; even the versions with no stimulus, or visual stimulus on the phone screen, got more positive results on this question.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/questionnaire_f.png}
	\caption{Answers for the question "It was easy to find what I was looking for when switching my gaze between the two screens" for each of the four versions}\label{fig:questionnaire_f}
\end{figure}

\subsection{Performance results}
As mentioned in section \ref{subsec:performance_metrics}, two logs were generated throughout the test: one noting time stamps for different events on the game, and one noting data from the kinect regarding whether players looked at or away from the main screen. By comparing the two, it was possible to measure the amount of time it took for each player to look at their phone after it became their turn, as well as after they were engaged in combat. This reaction time was noted for each of these events, and an average reaction time was calculated for each of the four versions of the game. As can be seen in Figure \ref{fig:reaction_time}, participants switched their visual attention fastest when given tactile stimulus, at an average of 0,78 seconds. Players switched their visual attention slowest when given visual stimulus on the main screen.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/reaction_time.png}
	\caption{Average reaction time for the four versions of the game}\label{fig:reaction_time}
\end{figure}

It is, however, significant to note that in some cases, test participants failed to look at the phone screen at all during their turn or a combat they were a part of, and in other cases, participants were already looking at their phone by the time it became their turn or they engaged in combat. In these cases, it was impossible to note a reaction time. Instead, Figure \ref{fig:looked} illustrates the rate at which these two circumstances occur, for each of the four versions. As the figure shows, these cases occurred most frequently when players were given no indication of when it was their turn, and least so when they were given visual stimulus on the main screen. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{figures/graph_looked.png}
	\caption{The rate at which participants failed to look at their phone screen, or were already looking at it, for each of the four versions}\label{fig:looked}
\end{figure}

\section{Analysis and Conclusion}
The results from the questionnaires were set up in a table for each of the statements described in Section \ref{subsec:user_experience_metrics}, and each possible answer was given a value ranging from 1-7, where 1 corresponds to a "strongly disagree" response, and 7 corresponds to "strongly agree". This was done to make it possible to use a Wilcoxon ranked sum test, to see if there is a significant difference between the game version with no feedback and the game versions with feedback. In this case, the version with no feedback (labeled A in Chapter \ref{ch:game}) serves as a control version. The Wilcoxon ranked sum test fits the test since it aims to find differences between two paired samples with a nominal predictor variable (the test version), and a single ordinal response variable (the questionnaire reponses from the participants), along with the fact that the data is non-parametric with a sample size below 30. The Wilcoxon test is made for each statement in the questionnaire, where each non-control game version is compared to the control sample. The p-value for each of these 18 tests are seen in Table \ref{tbl:p_table}. All of the cases where p < 0.05 are marked in \textbf{bold}.
 

\begin{center}
  \begin{tabular*}{\textwidth}{ | p{4.84cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} |}
    \hline
    Game version & B) Tactile & C) Main screen visual & D) Phone screen visual \\ \hline
    It was clear when it  was my turn to act & \textbf{p = 0.0006928} & \textbf{p = 0.00006759} & \textbf{p = 0.009489} \\ \hline
    It was clear when I was in combat & \textbf{p = 0.0133} & \textbf{p = 0.005767} & p = 0.5524 \\ \hline
    It was clear when I was supposed to look at the phone screen & \textbf{p = 0.00494} & \textbf{p = 0.00001718} & p = 0.16 \\ \hline
    It was clear when I was supposed to look at the main screen & p = 0.1028 & \textbf{p = 0.0002686} & p = 0.3058 \\ \hline
    It was easy for me to switch my attention between the two screens & \textbf{p = 0.007815} & \textbf{p = 0.04137} & p = 0.3266 \\ \hline
    It was easy to find what I was looking for when switching my gaze between the screens & p = 0.05888 & \textbf{p = 0.04512} & p = 0.1975 \\
    \hline
  \end{tabular*}\captionof{table}{p values for the hypothesis: "Test participants agree with the statement to a higher degree for this version of the game than for the control version", for each statement and each non-control game version}\label{tbl:p_table}
\end{center}

When looking at the p-values from version B (the tactile version), it is clear that the tactile feedback helped the participants in noticing when it was their turn, when they entered combat, and when to look at their phone compared to the control version. It also helped them to switch their attention between the screens. However, this feedback type was lacking when it came to directing the attention to the main screen. 
The p-values from version C (the main screen version), show that all of the hypotheses are true, and that the null-hypothesis --- which is "test participants agree with the statement to a lesser or equal degree for this version of the game compared to the control version" --- can be rejected. This means that this type of feedback works significantly better than the control sample in all areas addressed by the questionnaire.
The results for version D (the phone screen version) show a significant improvement compared to the control version only in one area; communicating to the participants when it was their turn to act. This indicates that it did not help direct attention as well as the other two versions. 

From these results, it can be seen that version C in general performs better when it comes to user experience metrics than other types of feedback. The only statement in which another type of feedback performs better is "it was easy for me to switch my attention between the two screens", in which version B yields a much lower p-value. The version with the poorest user-experience results is version D, where only 1 of the 6 p-values are below the 0.05 margin.

When looking at the performance metric results, it is important to note that even though the game version with visual stimulus on the main screen caused test participants to perform the worst with regards to reaction time, it is the version which caused fewest cases where participants were already looking at the screen to wait for their turn, whereas the version with no indication of whose turn it is caused relatively faster reactions from participants, but many would constantly look at their phone screen, even if it was not their turn, which is also not ideal. This is possibly because they were anticipating their turn, but since they would be given no indication of when their turn would start, they had to check in with their phone screen more often. This would cause them to look at the main screen less. Version B performed the best with regards to reaction time, as it is the only version in which participants on average switched their gaze from the main screen to the phone screen in less that a second.

\section{Discussion of results}
Even though the evaluation gave data to draw conclusions from, there is a need to take certain aspects into account that might have influenced the validity and reliability of results both in a positive and negative manner. 

\subsection{Reliability}
The test was designed in such a way that each participant tested all of the four versions of the game. This can be viewed as a positive aspect of the evaluation setup, since in that way the feedback given on all of the game versions did not depend on the fact that each version was viewed by different people.

On the negative side, however,  the test was conducted only among third and fifth semester Medialogy students. In addition, it would have been a good idea to have more participants since there were four versions of the game, and even though the order in which the versions were played was randomised between the groups, in order to test every possible playing order, 24 tests in groups of four participants should have been conducted. This would sum up to involving 96 participants, which would not be possible to do considering the time constraints. 
Furthermore, time measurements, both in Kinect and game log, did not include milliseconds, since it would be difficult to have the exact same time on two computers. The solution to this issue could have been to implement the Kinect code within the game software, which was not possible to do due to time constraints.

\subsection{Validity}
There were several factors that influenced validity in a positive way, which include having different groups of participants play the four versions of the game in a randomised order and providing a test environment that resembles a home setup. The latter may have influenced the participantsâ€™ level of comfort and helped reduce the background noise level. 

However, the results may have been affected in a negative way due to some technical issues. For instance, there was latency during combat, and in a few cases, the necessary buttons did not appear at once. Also, the tactile feedback was not strong enough on some of the phones during the test according to some participants, thus making those tactile results invalid. It may have been ideal for the vibrations to last longer, allowing more participants to notice them.

Furthermore, there were four different models of phones used for the evaluation, with one of the phones being switched during the last three tests, therefore influencing the consistency of the evaluation. 

Another source of error might have occurred due to the results obtained from Kinect, since it might not have been sensitive enough for the purpose of the evaluation testing. Besides that, the software may have logged that a participant was looking away when he or she was looking at the main screen. This could have happened due to the position of the hardware, which was right below the main screen, so the participants did not look directly on the Kinect camera, but at a point above it. 

It could have also been a good idea to log participantâ€™s mistakes, for example when a move was chosen that a participant did not intend to make. This could have given more data to take into consideration and feedback on the game itself. Furthermore, tracking what specifically were participants concentrating upon when looking at the main screen could have given us a better understanding of how the attention of the participants was directed. 

\section{In conclusion}
When considering the results from both the user experience and the performance tests, it is clear that in several aspects, a visual cue on the shared screen is a viable way to direct players' attention between two screens in a setup such as that of the AirConsole platform. This stands in contrast to a visual stimulus on the phone screen, which shows little indication that it helps players in direction their attention to the relevant screen. However, tactile stimulus also shows potential in directing players' attention, especially if the situation warrants a quick reaction. Since tactile and visual stimuli are not in direct conflict with each other, a viable solution may be to combine phone vibration with a visual cue on the main screen. This project does not investigate the performance of such a solution, and thus a final conclusion on that subject would need further research. This, as well as additional potential for further research, will be discussed in the following chapter.